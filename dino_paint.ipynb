{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9282643-c6fa-4806-8a91-0156710c8664",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with convpaint and DINOv2\n",
    "\n",
    "This notebooks demonstrates how to run a semantic segmentation on an image using DINOv2 for feature extraction and a random forest algorithm for classification. It is based on the notebook provided by convpaint and runs independently from napari.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a1c36c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "e314538e-f42d-42d0-89a9-4ddd4588c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import napari and its screenshot function\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# import what we need from conv_paint\n",
    "from napari_convpaint.conv_paint import ConvPaintWidget\n",
    "# from napari_convpaint.conv_paint_utils import Hookmodel\n",
    "from napari_convpaint.convpaint_sample import create_annotation_cell3d\n",
    "from napari_convpaint.conv_paint_utils import (filter_image_multioutputs, get_features_current_layers,\n",
    "get_multiscale_features, train_classifier, predict_image)\n",
    "from napari_convpaint.conv_paint_utils import extract_annotated_pixels\n",
    " \n",
    "# import the other general modules used\n",
    "import numpy as np\n",
    "import skimage\n",
    "# import tifffile\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import pytorch and pillow Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, ToPILImage, InterpolationMode, CenterCrop\n",
    "from PIL import Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1570ce97",
   "metadata": {},
   "source": [
    "## Define the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = (14, 14)\n",
    "crop_to_patch = True\n",
    "scale = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02abf123-f8da-4c15-8dc4-79e421f843a0",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load an image and the corresponding annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "88dcd229-c9fe-4d4a-8df9-9f52cc4dc353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (512, 512, 3)\n",
      "Original label image shape: (512, 512)\n"
     ]
    }
   ],
   "source": [
    "# LOAD CELL3D IMAGE AND ANNOTATION\n",
    "# Load 3D image with 2 channels (cell borders and nuclei)\n",
    "# image_original = skimage.data.cells3d()\n",
    "# Take a layer in middle of cell (30 of 0-59) and take 2nd channel (nuclei)\n",
    "# image_original = image_original[30, 1]\n",
    "# Load annotation defined in conv_paint\n",
    "# labels_original = create_annotation_cell3d()[0][0]\n",
    "\n",
    "# Take crops of image and annotation\n",
    "# crop = ((60,188), (0,128))\n",
    "# crop = ((20,20+224), (0,224))\n",
    "# image_original = image_original[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "# labels_original = labels_original[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "\n",
    "# LOAD ASTRONAUT IMAGE (RGB) AND ANNOTATION\n",
    "image_original = skimage.data.astronaut()#[0:504,0:504,:]\n",
    "labels_original = plt.imread('astro_labels_2.tif')[:,:,0]#[0:504,0:504]\n",
    "\n",
    "# PRINT SHAPES\n",
    "print(f\"Original image shape: {image_original.shape}\")\n",
    "print(f\"Original label image shape: {labels_original.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c288bc18",
   "metadata": {},
   "source": [
    "Show the image and annotation as layers in napari. Print out the shape of the original image and the number of originally annotated pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "88304336-c827-45d9-a27d-b554b8c486e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a napari viewer; add the image to it; add the labels/annotation\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.add_image(image_original)\n",
    "# viewer.add_labels(labels_original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe19519-c478-4184-b223-b448ef434c93",
   "metadata": {},
   "source": [
    "We can also show a napari screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "c829a161-6e6f-4e10-ba33-439b038010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot of the napari viewer here in the notebook\n",
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "11df1957-bfec-4cba-861d-b5f38380fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tifffile.imwrite('label_cell3d.tiff', viewer.layers['Labels'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8380ba7e-eb62-4297-92bd-827fe5ab9b41",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "DINOv2 comes in 4 different versions, each increasing in training set size and power. Choose the desired model by assigning it to 'model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "b50a98ea-6939-45c4-be7d-1835b7a917be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37eee0a4",
   "metadata": {},
   "source": [
    "Define the model parameters and the processing of the image.\n",
    "Note that the patch size that DINOv2 uses is 14x14. Therefore, the number of patches is the input shape divided by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "32a691a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image is: 512 x 512 pixels\n",
      "Image is cropped to: 504 x 504 pixels\n",
      "Shape of input used for model (multiple of patch size): 504 x 504 pixels\n",
      "Patched image shape: 36 x 36 patches\n"
     ]
    }
   ],
   "source": [
    "# Define input shape as the smallest multiple of the patch size that is larger than the image, optionally scaled by a factor\n",
    "if crop_to_patch:\n",
    "    crop_shape = (int(np.floor(image_original.shape[0]/PATCH_SIZE[0]))*PATCH_SIZE[0],\n",
    "                  int(np.floor(image_original.shape[1]/PATCH_SIZE[1]))*PATCH_SIZE[1])\n",
    "    in_shape = (crop_shape[0] * scale, crop_shape[1] * scale)\n",
    "\n",
    "else:\n",
    "    crop_shape = image_original.shape\n",
    "    in_shape = (int(np.ceil(image_original.shape[0]/PATCH_SIZE[0]))*PATCH_SIZE[0] * scale,\n",
    "                int(np.ceil(image_original.shape[1]/PATCH_SIZE[1]))*PATCH_SIZE[1] * scale)\n",
    "\n",
    "image_cropped = image_original[0:crop_shape[0], 0:crop_shape[1]]\n",
    "labels_cropped = labels_original[0:crop_shape[0], 0:crop_shape[1]]\n",
    "image_scaled = skimage.transform.resize(image_cropped, in_shape, mode='edge', order=1, preserve_range=True)\n",
    "labels_scaled = skimage.transform.resize(labels_cropped, in_shape, mode='edge', order=0, preserve_range=True)\n",
    "\n",
    "# Calculate the shape of the patched image (i.e. how many patches fit in the image)\n",
    "if not (in_shape[0]%PATCH_SIZE[0] == 0 and in_shape[1]%PATCH_SIZE[1] == 0):\n",
    "    raise ValueError('Input shape must be divisible by patch size')\n",
    "else:\n",
    "    patched_image_shape = (int(in_shape[0]/PATCH_SIZE[0]), int(in_shape[1]/PATCH_SIZE[1]))\n",
    "\n",
    "\n",
    "print(f\"Original image is: {image_original.shape[0]} x {image_original.shape[1]} pixels\")\n",
    "print(f\"Image is cropped to: {crop_shape[0]} x {crop_shape[1]} pixels\")\n",
    "print(f\"Shape of input used for model (multiple of patch size): {in_shape[0]} x {in_shape[1]} pixels\")\n",
    "print(f\"Patched image shape: {patched_image_shape[0]} x {patched_image_shape[1]} patches\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5201b9",
   "metadata": {},
   "source": [
    "## Convert & preprocess image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8a79f-752a-4ac9-b0e9-2bb485b75338",
   "metadata": {},
   "source": [
    "Resize the image to the input shape (which has to be a multiple of the patch size 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "912f6186-b239-42cd-87ec-b63907c69c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale original image to input size of the model\n",
    "image_scaled = image_scaled.astype(np.float32)\n",
    "labels_scaled = labels_scaled.astype(np.int32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f76478",
   "metadata": {},
   "source": [
    "Show the scaled version of the image and labels to ensure that it still looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "970f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewer = napari.Viewer()\n",
    "# viewer.add_image(image_scaled.astype(np.int32))\n",
    "# viewer.add_labels(labels_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bee4328",
   "metadata": {},
   "source": [
    "Convert the image to RGB. Then preprocess it into a torch tensor, normalized according to distribution expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "10df465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RGB\n",
    "if image_scaled.ndim == 2:\n",
    "    image_rgb = np.stack((image_scaled,)*3, axis=-1)\n",
    "else:\n",
    "    image_rgb = image_scaled\n",
    "\n",
    "# New shape is (224, 224, 3); type is float64\n",
    "# print(image_rgb.shape); print(image_rgb.dtype)\n",
    "# napari.view_image(image_rgb.astype(np.int64), rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "90f3fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_mean, current_sd = np.mean(image_rgb, axis=(0,1)), np.std(image_rgb, axis=(0,1))\n",
    "new_mean, new_sd = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n",
    "image_norm = (image_rgb - current_mean) / current_sd\n",
    "image_norm = image_norm * new_sd + new_mean\n",
    "\n",
    "# Check that it worked\n",
    "# print(np.mean(image_rgb, axis=(0,1)))\n",
    "# print(np.mean(image_norm, axis=(0,1)))\n",
    "\n",
    "# Show normalized image\n",
    "# napari.view_image(image_norm, rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "f87a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor\n",
    "image_tensor = ToTensor()(image_norm).float()\n",
    "\n",
    "# Preprocess image and convert to PyTorch tensor\n",
    "# preprocess = Compose([\n",
    "#     ToTensor(),  # Convert to PyTorch tensor\n",
    "#     # Resize(in_shape, interpolation=InterpolationMode.BILINEAR, antialias=True),  # Resize to the input size expected by the model\n",
    "#     # CenterCrop(in_shape),  # Crop to the input size expected by the model\n",
    "#     # ToTensor(),  # Convert to PyTorch tensor\n",
    "#     # Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalize to the input distribution expected by the model\n",
    "# ])\n",
    "\n",
    "# image_tensor = preprocess(image_norm).float()\n",
    "# image_tensor = preprocess(image_rgb)\n",
    "\n",
    "# image_tensor = np.array(image_tensor)\n",
    "# image_tensor = preprocess(Image.fromarray(image_rgb.astype(np.uint8))).float()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7014e59",
   "metadata": {},
   "source": [
    "Show output of preprocessing to verify it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "3830d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Mean: {image_tensor.mean(dim=(1, 2))}\")\n",
    "# print(f\"Standard Deviation: {image_tensor.std(dim=(1, 2))}\")\n",
    "# np.mean(image_tensor, axis=(1,2))\n",
    "# print(image_rgb[0:4,0:4,:])\n",
    "# print(image_tensor.numpy().transpose(1,2,0)[0:4,0:4,:])\n",
    "\n",
    "# tensor_np = image_tensor.numpy().transpose(1,2,0)\n",
    "# print(tensor_np.shape)\n",
    "# napari.view_image(tensor_np, rgb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cb5ea6-ba0d-4281-932e-4be4bc73abd7",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now that the model is defined, we can run an image through it and extract features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "662e0575-f0a3-4cf1-87b4-6e7365273830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra batch dimension \n",
    "image_batch = image_tensor.unsqueeze(0)\n",
    "\n",
    "# Move image to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# image_batch = image_batch.to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# Pass image through the model (assuming image_batch is a batch of test images)\n",
    "with torch.no_grad():\n",
    "    features_dict = model.forward_features(image_batch)\n",
    "    features = features_dict['x_norm_patchtokens']\n",
    "\n",
    "# The output shape is [batch_size, num_patches, features] = [1, 256, NUM_FEATURES]\n",
    "# print(features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2cf84",
   "metadata": {},
   "source": [
    "Rearrange and reshape dimensions of the DINOv2 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number features: 384\n"
     ]
    }
   ],
   "source": [
    "# Read out the number of features, which is dependent on the model chosen (ViTs14 = 384; ViTb14 = 768; ViTl14 = 1024; ViTg14 = 1536)\n",
    "NUM_FEATURES = features.shape[2]\n",
    "print(f\"Number features: {NUM_FEATURES}\")\n",
    "\n",
    "# Rearrange dimensions of the feature tensor to [batch_size, features, num_patches] = [1, NUM_FEATURES, 256]\n",
    "features_perm = features.permute(0, 2, 1)\n",
    "# print(features.shape)\n",
    "\n",
    "# Reshape linear patches (256) into 2D: [batch_size, features, patches_w, patches_h] = [1, NUM_FEATURES, 16, 16]\n",
    "features_wh = features_perm.reshape(1, NUM_FEATURES, patched_image_shape[0], patched_image_shape[1])\n",
    "# print(features_wh.shape)\n",
    "\n",
    "# Upsample to original image size, i.e. [batch_size, features, image_w, image_h] = [1, NUM_FEATURES, 128, 128] or [1, NUM_FEATURES, 224, 224]\n",
    "# scaling_factor = (14 * image_original.shape[0] / 224, 14 * image_original.shape[1] / 224)\n",
    "\n",
    "# Upsample to the size of the scaled image (i.e. interpolate with scaling factor = patch_size = 14)\n",
    "features_int = torch.nn.functional.interpolate(features_wh, scale_factor=PATCH_SIZE)\n",
    "# print(features_int.shape)\n",
    "\n",
    "# Convert to numpy array and remove batch dimension to get [features, image_w, image_h] = [NUM_FEATURES, 128, 128] or [NUM_FEATURES, 224, 224]\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)\n",
    "# print(f\"Shape of features_np: {features_np.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffd8243",
   "metadata": {},
   "source": [
    "Show feature space in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "11565b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the loaded image and the annotation\n",
    "# viewer = napari.Viewer()\n",
    "# viewer.add_image(image_scaled.astype(np.int32))\n",
    "# viewer.add_labels(labels_scaled)\n",
    "# # add the feature space\n",
    "# viewer.add_image(features_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53809e60",
   "metadata": {},
   "source": [
    "## Train and use Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "500ba0d1",
   "metadata": {},
   "source": [
    "Extract features and target values (labels) where image is annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "15bf7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of annotated features: (5744, 384)\n",
      "Number of targets: 5744\n",
      "Number of originally annotated pixels: 8789\n",
      "Number of annotated pixels in resized annotation: 8761\n"
     ]
    }
   ],
   "source": [
    "features_annot, targets = extract_annotated_pixels(features_np, labels_scaled, full_annotation=False)\n",
    "# features.shape = (646, NUM_FEATURES)\n",
    "# targets.shape = (646,)\n",
    "print(f\"Shape of annotated features: {features_annot.shape}\")\n",
    "print(f\"Number of targets: {targets.shape[0]}\")\n",
    "print(f\"Number of originally annotated pixels: {sum(labels_original[labels_original>0])}\")\n",
    "print(f\"Number of annotated pixels in resized annotation: {sum(labels_scaled[labels_scaled>0])}\")\n",
    "\n",
    "# # NOTE: in convpaint, we had\n",
    "# features.shape = (218, 640)\n",
    "# targets.shape = (218,)\n",
    "# And the number of originally annotated pixels was 327"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "107782db",
   "metadata": {},
   "source": [
    "Train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "0faa52a6-b66d-4775-b58e-92b7ab13f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_classifier(features_annot, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2ed1f9-9091-4c4f-b85d-b199c8939eba",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "93cb3004-7780-496a-b499-bc89b24bbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If we wanted to predict on another image, we would have to do the following:\n",
    "# 1) Extract features from the new image using the same DINOv2 model\n",
    "# 2) predict on the features using the random forest created above (learned from the original image)\n",
    "\n",
    "# Convert features to numpy array\n",
    "features_to_predict = features.numpy()\n",
    "# Remove the batch dimension\n",
    "features_to_predict_lin = features_to_predict.squeeze(0)\n",
    "\n",
    "# Run predict of random forest on all features\n",
    "predictions = random_forest.predict(features_to_predict_lin)\n",
    "\n",
    "# We have 256 predictions, which corresponds to the 256 patches (16x16 in the image)\n",
    "# print(predictions.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d36fc94",
   "metadata": {},
   "source": [
    "Reshape and resize the predictions so we can show and overlay them on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "e234f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the predictions to the shape of the image of patches\n",
    "predicted_image = predictions.reshape(patched_image_shape[0], patched_image_shape[1])\n",
    "# Resize to the size of the scaled input image\n",
    "predicted_image = skimage.transform.resize(predicted_image, in_shape, mode='edge', order=0, anti_aliasing=False)\n",
    "# Transform interpolated values to integer values\n",
    "predicted_image = predicted_image.astype(np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a81e79-8988-4191-a67f-344c7822a4f0",
   "metadata": {},
   "source": [
    "And finally we can visualize the output (and quantify its quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "129bfefb-6769-4faf-b614-e7441f7bf9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'predicted_image' at 0x2c07f3b19d0>"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled.astype(np.int32))\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the prediction\n",
    "viewer.add_labels(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "d64c0ddc-fac6-4509-9f10-60b0d86a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42430d5",
   "metadata": {},
   "source": [
    "## Tests Roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "39d55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CREATE AND SHOW FULL OUTPUT OF A LAYER OF VGG16 (= 64 FEATURES)\n",
    "# def get_layer_features(image, layer, show_napari = False, interpolate = False):\n",
    "        \n",
    "#     model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "\n",
    "#     all_layers = [key for key in model.module_dict.keys()]\n",
    "#     # Choose just 1 layer, and register a hook there\n",
    "#     if isinstance(layer, str):\n",
    "#         layers = [layer]\n",
    "#     elif isinstance(layer, int):\n",
    "#         layers = [all_layers[layer]]\n",
    "    \n",
    "#     # layers = ['features.30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) avgpool AdaptiveAvgPool2d(output_size=(7, 7))']\n",
    "#     model.register_hooks(selected_layers=layers)\n",
    "\n",
    "#     # Get features using only this first layer and without scaling\n",
    "#     features, targets = get_features_current_layers(\n",
    "#         model=model, image=image, annotations=image, scalings=[1], use_min_features=False, order=interpolate)\n",
    "\n",
    "#     # Convert the DataFrame to a numpy array\n",
    "#     features_array = features.values\n",
    "#     # Get the shape of the image\n",
    "#     image_shape = image.shape\n",
    "#     # Reshape the features array to match the image shape and add the second dimension of features as the third dimension\n",
    "#     features_image = features_array.reshape(*image_shape, -1)\n",
    "\n",
    "#     # Move the last dimension to the first position\n",
    "#     features_image = np.moveaxis(features_image, -1, 0)\n",
    "#     # print(features.shape)\n",
    "#     # print(features_image.shape)\n",
    "\n",
    "#     # Now you can view the new_features using napari\n",
    "#     if show_napari: napari.view_image(features_image)\n",
    "#     return features_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "id": "6e358c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN\n",
    "\n",
    "# # image = image.T\n",
    "\n",
    "# # Get features of multiple (all) layers\n",
    "# conv_layers = [0,2]#,5,7,10,12,14,17,19,21,24,26,28]\n",
    "# all_conv = [get_layer_features(image, l) for l in conv_layers]\n",
    "\n",
    "\n",
    "# ### Pad first dimension of the layers with fewer features and concatenate all layers into a 4D Image\n",
    "\n",
    "# # Get the shapes of all outputs\n",
    "# shapes = [output.shape for output in all_conv]\n",
    "# # Find the maximum shape in each dimension\n",
    "# max_shape = np.max(shapes, axis=0)\n",
    "# # Pad all outputs to have the max shape\n",
    "# from numpy.lib import pad\n",
    "# all_conv_padded = np.array([pad(output, [(0, max_dim - dim) for dim, max_dim in zip(output.shape, max_shape)]) for output in all_conv])\n",
    "\n",
    "# # Show in Napari\n",
    "# napari.view_image(all_conv_padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
