{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9282643-c6fa-4806-8a91-0156710c8664",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with convpaint and DINOv2\n",
    "\n",
    "This notebooks demonstrates how to run a semantic segmentation on an image using DINOv2 for feature extraction and a random forest algorithm for classification. It is based on the notebook provided by convpaint and runs independently from napari.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a1c36c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e314538e-f42d-42d0-89a9-4ddd4588c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import napari and its screenshot function\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# import what we need from conv_paint\n",
    "from napari_convpaint.conv_paint import ConvPaintWidget\n",
    "# from napari_convpaint.conv_paint_utils import Hookmodel\n",
    "from napari_convpaint.convpaint_sample import create_annotation_cell3d\n",
    "from napari_convpaint.conv_paint_utils import (filter_image_multioutputs, get_features_current_layers,\n",
    "get_multiscale_features, train_classifier, predict_image)\n",
    "from napari_convpaint.conv_paint_utils import extract_annotated_pixels\n",
    " \n",
    "# import the other general modules used\n",
    "import numpy as np\n",
    "import skimage\n",
    "# import tifffile\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import pytorch and pillow Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "# from PIL import Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02abf123-f8da-4c15-8dc4-79e421f843a0",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load an image and the corresponding annotation. Both are cropped to be 128x128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88dcd229-c9fe-4d4a-8df9-9f52cc4dc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3D image with 2 channels (cell borders and nuclei)\n",
    "image_original = skimage.data.cells3d()\n",
    "# Take a layer in middle of cell (30 of 0-59) and take 2nd channel (nuclei)\n",
    "image_original = image_original[30, 1]\n",
    "# Load annotation defined in conv_paint\n",
    "labels_original = create_annotation_cell3d()[0][0]\n",
    "\n",
    "# Take crops of image and annotation\n",
    "crop = ((60,188), (0,128))\n",
    "image_original = image_original[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "labels_original = labels_original[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "image_original_shape = image_original.shape\n",
    "\n",
    "# The original image 'cells3d' is 128x128 pixels\n",
    "# print(original_im_shape)\n",
    "# The number of annotated pixels is 327\n",
    "# print(sum(originaÂ§l_labels[original_labels>0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c288bc18",
   "metadata": {},
   "source": [
    "Show the image and annotation as layers in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88304336-c827-45d9-a27d-b554b8c486e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labels_original' at 0x1677f6ad2e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a napari viewer; add the image to it; add the labels/annotation\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image_original)\n",
    "viewer.add_labels(labels_original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe19519-c478-4184-b223-b448ef434c93",
   "metadata": {},
   "source": [
    "We can also show a napari screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c829a161-6e6f-4e10-ba33-439b038010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot of the napari viewer here in the notebook\n",
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11df1957-bfec-4cba-861d-b5f38380fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tifffile.imwrite('label_cell3d.tiff', viewer.layers['Labels'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8380ba7e-eb62-4297-92bd-827fe5ab9b41",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "DINOv2 comes in 4 different versions, each increasing in training set size and power. Choose the desired model by assigning it to 'model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b50a98ea-6939-45c4-be7d-1835b7a917be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37eee0a4",
   "metadata": {},
   "source": [
    "Define the model parameters. Note that the patch size that DINOv2 uses is 14x14. Therefore, the number of patches is the input shape divided by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a691a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the patch size and input shape as global constants\n",
    "PATCH_SIZE = (14, 14)\n",
    "IN_SHAPE = (224, 224)\n",
    "# Calculate the shape of the patched image (i.e. how many patches fit in the image)\n",
    "if not (IN_SHAPE[0]%PATCH_SIZE[0] == 0 and IN_SHAPE[1]%PATCH_SIZE[1] == 0):\n",
    "    raise ValueError('Input shape must be divisible by patch size')\n",
    "else:\n",
    "    patched_image_shape = (int(IN_SHAPE[0]/PATCH_SIZE[0]), int(IN_SHAPE[1]/PATCH_SIZE[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5201b9",
   "metadata": {},
   "source": [
    "## Convert & preprocess image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8a79f-752a-4ac9-b0e9-2bb485b75338",
   "metadata": {},
   "source": [
    "Resize the image to the input shape (which has to be a multiple of the patch size 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "912f6186-b239-42cd-87ec-b63907c69c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale original image to input size of the model\n",
    "image_scaled = skimage.transform.resize(image_original, IN_SHAPE, mode='edge', order=1, preserve_range=True)\n",
    "labels_scaled = skimage.transform.resize(labels_original, IN_SHAPE, mode='edge', order=0, preserve_range=True)\n",
    "\n",
    "# The number of annotated pixels in the resized annotation is 994\n",
    "# print(sum(labels_scaled[labels_scaled>0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f76478",
   "metadata": {},
   "source": [
    "Show the scaled version of the image and labels to ensure that it still looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970f4d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labels_scaled' at 0x16774c2a4c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image_scaled)\n",
    "viewer.add_labels(labels_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bee4328",
   "metadata": {},
   "source": [
    "Convert the image to RGB. Then preprocess it into a torch tensor, normalized according to distribution expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f87a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RGB\n",
    "image_rgb = np.stack((image_scaled,)*3, axis=-1)\n",
    "napari.imshow(image_scaled)\n",
    "\n",
    "# New shape is (224, 224, 3)\n",
    "# print(image_rgb.shape)\n",
    "\n",
    "# Preprocess image\n",
    "preprocess = Compose([\n",
    "    #Resize((224, 224)),  # Resize to the input size expected by the model\n",
    "    ToTensor(),  # Convert to PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to the input distribution expected by the model\n",
    "])\n",
    "# TODO: Normalization with [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] OK?\n",
    "\n",
    "image_pre = preprocess(image_rgb).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cb5ea6-ba0d-4281-932e-4be4bc73abd7",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now that the model is defined, we can run an image through it and extract features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662e0575-f0a3-4cf1-87b4-6e7365273830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra batch dimension \n",
    "image_batch = image_pre.unsqueeze(0)\n",
    "\n",
    "# Move image to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# image_batch = image_batch.to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# Pass image through the model (assuming images is a batch of test images)\n",
    "with torch.no_grad():\n",
    "    # features=model.forward_features(torch.nn.functional.interpolate(images,(448,448)))['x_norm_patchtokens']\n",
    "    features=model.forward_features(image_batch)['x_norm_patchtokens']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2cf84",
   "metadata": {},
   "source": [
    "Rearrange and reshape dimensions of the DINOv2 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read out the number of features, which is dependent on the model chosen (e.g. small = 384; large = 1024)\n",
    "NUM_FEATURES = features.shape[2]\n",
    "\n",
    "# The output shape is [batch_size, num_patches, features] = [1, 256, NUM_FEATURES]\n",
    "# print(features.shape)\n",
    "\n",
    "# Rearrange dimensions of the feature tensor to [batch_size, features, num_patches] = [1, NUM_FEATURES, 256]\n",
    "features_perm = features.permute(0, 2, 1)\n",
    "# print(features.shape)\n",
    "\n",
    "# Reshape linear patches (256) into 2D: [batch_size, features, patches_w, patches_h] = [1, NUM_FEATURES, 16, 16]\n",
    "features_wh = features_perm.reshape(1, NUM_FEATURES, patched_image_shape[0], patched_image_shape[1])\n",
    "# print(features_wh.shape)\n",
    "\n",
    "# Upsample to original image size, i.e. [batch_size, features, image_w, image_h] = [1, NUM_FEATURES, 128, 128] or [1, NUM_FEATURES, 224, 224]\n",
    "# scaling_factor = (14 * image_original.shape[0] / 224, 14 * image_original.shape[1] / 224)\n",
    "\n",
    "# Upsample to the size of the scaled image (i.e. interpolate with scaling factor = patch_size = 14)\n",
    "features_int = torch.nn.functional.interpolate(features_wh, scale_factor=PATCH_SIZE)\n",
    "# print(features_int.shape)\n",
    "\n",
    "# Convert to numpy array and remove batch dimension to get [features, image_w, image_h] = [NUM_FEATURES, 128, 128] or [NUM_FEATURES, 224, 224]\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)\n",
    "# print(features_np.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffd8243",
   "metadata": {},
   "source": [
    "Show feature space in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11565b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'features_np' at 0x1c7fb37aee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the feature space\n",
    "viewer.add_image(features_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "500ba0d1",
   "metadata": {},
   "source": [
    "Extract features and target values (labels) where image is annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15bf7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646, 1024)\n",
      "(646,)\n"
     ]
    }
   ],
   "source": [
    "features_annot, targets = extract_annotated_pixels(features_np, labels_scaled)\n",
    "# features.shape = (646, NUM_FEATURES)\n",
    "# targets.shape = (646,)\n",
    "print(features_annot.shape)\n",
    "print(targets.shape)\n",
    "# # NOTE: in convpaint, we had\n",
    "# features.shape = (218, 640)\n",
    "# targets.shape = (218,)\n",
    "# And the number of annotated pixels was 327"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53809e60",
   "metadata": {},
   "source": [
    "## Train and use Classifier\n",
    "Finally we can train a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0faa52a6-b66d-4775-b58e-92b7ab13f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_classifier(features_annot, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2ed1f9-9091-4c4f-b85d-b199c8939eba",
   "metadata": {},
   "source": [
    "And do a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93cb3004-7780-496a-b499-bc89b24bbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy array\n",
    "all_features = features.numpy()\n",
    "# Remove the batch dimension\n",
    "all_features_lin = all_features.squeeze(0)\n",
    "\n",
    "# Run predict of random forest on all features\n",
    "predictions = random_forest.predict(all_features_lin)\n",
    "\n",
    "# We have 256 predictions, which corresponds to the 256 patches (16x16 in the image)\n",
    "# print(predictions.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d36fc94",
   "metadata": {},
   "source": [
    "Reshape and resize the predictions so we can show and overlay them on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the predictions to the shape of the image of patches\n",
    "predicted_image = predictions.reshape(patched_image_shape[0], patched_image_shape[1])\n",
    "# Resize to the size of the scaled input image\n",
    "predicted_image = skimage.transform.resize(predicted_image, IN_SHAPE, mode='edge', order=0, anti_aliasing=False)\n",
    "# Transform interpolated values to integer values\n",
    "predicted_image = predicted_image.astype(np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a81e79-8988-4191-a67f-344c7822a4f0",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "And finally we can visualize the output (and quantify its quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "129bfefb-6769-4faf-b614-e7441f7bf9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'predicted_image' at 0x1c80b69cbe0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the prediction\n",
    "viewer.add_labels(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c0ddc-fac6-4509-9f10-60b0d86a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42430d5",
   "metadata": {},
   "source": [
    "## Tests Roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AND SHOW FULL OUTPUT OF A LAYER OF VGG16 (= 64 FEATURES)\n",
    "def get_layer_features(image, layer, show_napari = False, interpolate = False):\n",
    "        \n",
    "    model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "\n",
    "    all_layers = [key for key in model.module_dict.keys()]\n",
    "    # Choose just 1 layer, and register a hook there\n",
    "    if isinstance(layer, str):\n",
    "        layers = [layer]\n",
    "    elif isinstance(layer, int):\n",
    "        layers = [all_layers[layer]]\n",
    "    \n",
    "    # layers = ['features.30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) avgpool AdaptiveAvgPool2d(output_size=(7, 7))']\n",
    "    model.register_hooks(selected_layers=layers)\n",
    "\n",
    "    # Get features using only this first layer and without scaling\n",
    "    features, targets = get_features_current_layers(\n",
    "        model=model, image=image, annotations=image, scalings=[1], use_min_features=False, order=interpolate)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    features_array = features.values\n",
    "    # Get the shape of the image\n",
    "    image_shape = image.shape\n",
    "    # Reshape the features array to match the image shape and add the second dimension of features as the third dimension\n",
    "    features_image = features_array.reshape(*image_shape, -1)\n",
    "\n",
    "    # Move the last dimension to the first position\n",
    "    features_image = np.moveaxis(features_image, -1, 0)\n",
    "    # print(features.shape)\n",
    "    # print(features_image.shape)\n",
    "\n",
    "    # Now you can view the new_features using napari\n",
    "    if show_napari: napari.view_image(features_image)\n",
    "    return features_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e358c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# image = image.T\n",
    "\n",
    "# Get features of multiple (all) layers\n",
    "conv_layers = [0,2]#,5,7,10,12,14,17,19,21,24,26,28]\n",
    "all_conv = [get_layer_features(image, l) for l in conv_layers]\n",
    "\n",
    "\n",
    "### Pad first dimension of the layers with fewer features and concatenate all layers into a 4D Image\n",
    "\n",
    "# Get the shapes of all outputs\n",
    "shapes = [output.shape for output in all_conv]\n",
    "# Find the maximum shape in each dimension\n",
    "max_shape = np.max(shapes, axis=0)\n",
    "# Pad all outputs to have the max shape\n",
    "from numpy.lib import pad\n",
    "all_conv_padded = np.array([pad(output, [(0, max_dim - dim) for dim, max_dim in zip(output.shape, max_shape)]) for output in all_conv])\n",
    "\n",
    "# Show in Napari\n",
    "napari.view_image(all_conv_padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
