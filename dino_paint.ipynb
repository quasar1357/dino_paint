{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9282643-c6fa-4806-8a91-0156710c8664",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with convpaint and DINOv2\n",
    "\n",
    "This notebooks demonstrates how to run a semantic segmentation on an image using DINOv2 for feature extraction and a random forest algorithm for classification. It is based on the notebook provided by convpaint and runs independently from napari.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a1c36c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e314538e-f42d-42d0-89a9-4ddd4588c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import napari and its screenshot function\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# import what we need from conv_paint\n",
    "from napari_convpaint.conv_paint import ConvPaintWidget\n",
    "from napari_convpaint.conv_paint_utils import Hookmodel\n",
    "from napari_convpaint.convpaint_sample import create_annotation_cell3d\n",
    "from napari_convpaint.conv_paint_utils import (filter_image_multioutputs, get_features_current_layers,\n",
    "get_multiscale_features, train_classifier, predict_image)\n",
    "from napari_convpaint.conv_paint_utils import extract_annotated_pixels\n",
    " \n",
    "# import the other general modules used\n",
    "import numpy as np\n",
    "import skimage\n",
    "import tifffile\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "# import pillow.image\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02abf123-f8da-4c15-8dc4-79e421f843a0",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load an image and the corresponding annotation. Both are cropped to be 128x128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88dcd229-c9fe-4d4a-8df9-9f52cc4dc353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "# Load 3D image with 2 channels (cell borders and nuclei)\n",
    "original_image = skimage.data.cells3d()\n",
    "# Take a layer in middle of cell (30 of 0-59) and take 2nd channel (nuclei)\n",
    "original_image = original_image[30,1]\n",
    "# Load annotation defined in conv_paint\n",
    "original_labels = create_annotation_cell3d()[0][0]\n",
    "\n",
    "# Take crops of image and annotation\n",
    "crop = ((60,188), (0,128))\n",
    "original_image = original_image[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "original_labels = original_labels[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "original_im_shape = original_image.shape\n",
    "# The original image 'cells3d' is 128x128 pixels\n",
    "# print(original_im_shape)\n",
    "# The number of annotated pixels is 327\n",
    "# print(sum(originaÂ§l_labels[original_labels>0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c288bc18",
   "metadata": {},
   "source": [
    "Show the image and annotation as layers in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88304336-c827-45d9-a27d-b554b8c486e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'original_labels' at 0x12eea1494c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a napari viewer\n",
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(original_image)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(original_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe19519-c478-4184-b223-b448ef434c93",
   "metadata": {},
   "source": [
    "Instead we can show a napari screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829a161-6e6f-4e10-ba33-439b038010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot of the napari viewer here in the notebook\n",
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df1957-bfec-4cba-861d-b5f38380fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tifffile.imwrite('label_cell3d.tiff', viewer.layers['Labels'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8380ba7e-eb62-4297-92bd-827fe5ab9b41",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "DINOv2 comes in 4 different versions, each increasing in training set size and power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b50a98ea-6939-45c4-be7d-1835b7a917be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "# dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# Choose the model to use\n",
    "model = dinov2_vits14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5201b9",
   "metadata": {},
   "source": [
    "## Convert & preprocess image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8a79f-752a-4ac9-b0e9-2bb485b75338",
   "metadata": {},
   "source": [
    "We convert the image to a pillow format RGB image. Then it is preprocessed into a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "912f6186-b239-42cd-87ec-b63907c69c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (224, 224)\n",
    "scaling_factor = (original_im_shape[0] / in_shape[0], original_im_shape[1] / in_shape[1])\n",
    "image_scaled = skimage.transform.resize(original_image, in_shape, mode='edge', order=1, anti_aliasing=True)\n",
    "labels_scaled = skimage.transform.resize(original_labels, in_shape, mode='edge', order=0, anti_aliasing=False)\n",
    "\n",
    "# create a napari viewer\n",
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "\n",
    "# Convert to pillow image and make RGB\n",
    "image_pillow = Image.fromarray(image_scaled)\n",
    "image_pillow = image_pillow.convert(\"RGB\")\n",
    "\n",
    "# Preprocess image\n",
    "preprocess = Compose([\n",
    "    # Resize((224, 224)),  # Resize to the input size expected by the model\n",
    "    ToTensor(),  # Convert to PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to ImageNet mean and std\n",
    "])\n",
    "image_pre = preprocess(image_pillow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cb5ea6-ba0d-4281-932e-4be4bc73abd7",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now that the model is defined, we can run an image through it and extract features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "662e0575-f0a3-4cf1-87b4-6e7365273830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra batch dimension and move image to the GPU if available\n",
    "images = image_pre.unsqueeze(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images = images.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Pass image through the model (assuming images is a batch of test images)\n",
    "with torch.no_grad():\n",
    "    # features=model.forward_features(torch.nn.functional.interpolate(images,(448,448)))['x_norm_patchtokens']\n",
    "    features=model.forward_features(images)['x_norm_patchtokens']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2cf84",
   "metadata": {},
   "source": [
    "Rearrange and reshape dimensions of the DINOv2 output. Note that the patch size that DINOv2 uses is 14x14. Therefore, the number of patches is the input shape divided by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 16, 16])\n",
      "torch.Size([1, 384, 224, 224])\n",
      "(384, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# The output shape is [batch_size, patches, features] = [1, 256, 384]\n",
    "# print(features.shape)\n",
    "\n",
    "# Rearrange dimensions of the feature tensor to [batch_size, features, patches] = [1, 384, 256]\n",
    "features_perm = features.permute(0,2,1)\n",
    "# print(features.shape)\n",
    "\n",
    "# Reshape linear patches (256) into width and height --> [1, 384, 16, 16]\n",
    "features_wh = features_perm.reshape(1,384,16,16)\n",
    "print(features_wh.shape)\n",
    "\n",
    "# Upsample to original image size, i.e. [batch_size, features, image_w, image_h] = [1, 384, 128, 128] or [1, 384, 224, 224]\n",
    "# fact = (14 * original_im_shape[0] / 224, 14 * original_im_shape[1] / 224)\n",
    "fact = (14, 14)\n",
    "features_int = torch.nn.functional.interpolate(features_wh, scale_factor=fact)\n",
    "print(features_int.shape)\n",
    "\n",
    "# Convert to numpy array and remove batch dimension to get [features, image_w, image_h] = [384, 128, 128] or [1, 384, 224, 224]\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)\n",
    "print(features_np.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffd8243",
   "metadata": {},
   "source": [
    "Show feature space in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11565b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the feature space\n",
    "viewer.add_image(features_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "500ba0d1",
   "metadata": {},
   "source": [
    "Extract features and target values (labels) where image is annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15bf7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(646, 384)\n",
      "(646,)\n"
     ]
    }
   ],
   "source": [
    "features_annot, targets = extract_annotated_pixels(features_np, labels_scaled)\n",
    "# features.shape = (646, 384)\n",
    "# targets.shape = (646,)\n",
    "print(features_annot.shape)\n",
    "print(targets.shape)\n",
    "# # NOTE: in convpaint, we had\n",
    "# features.shape = (218, 640)\n",
    "# targets.shape = (218,)\n",
    "# And the number of annotated pixels was 327"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53809e60",
   "metadata": {},
   "source": [
    "## Train and use Classifier\n",
    "Finally we can train a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0faa52a6-b66d-4775-b58e-92b7ab13f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_classifier(features_annot, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2ed1f9-9091-4c4f-b85d-b199c8939eba",
   "metadata": {},
   "source": [
    "And do a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93cb3004-7780-496a-b499-bc89b24bbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "all_features = features.numpy()\n",
    "all_features_lin = all_features.squeeze(0)\n",
    "\n",
    "predictions = random_forest.predict(all_features_lin)\n",
    "\n",
    "# We have 256 predictions, which corresponds to the 256 patches\n",
    "print(predictions.shape)\n",
    "\n",
    "predicted_image = predictions.reshape(16,16)#in_shape[0],in_shape[1])\n",
    "predicted_image = skimage.transform.resize(predicted_image, in_shape, mode='edge', order=0, anti_aliasing=False)\n",
    "predicted_image = predicted_image.astype(np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a81e79-8988-4191-a67f-344c7822a4f0",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "And finally we can visualize the output (and quantify its quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "129bfefb-6769-4faf-b614-e7441f7bf9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'predicted_image' at 0x12f61b51b80>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the prediction\n",
    "viewer.add_labels(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c0ddc-fac6-4509-9f10-60b0d86a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42430d5",
   "metadata": {},
   "source": [
    "## Tests Roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AND SHOW FULL OUTPUT OF A LAYER OF VGG16 (= 64 FEATURES)\n",
    "def get_layer_features(image, layer, show_napari = False, interpolate = False):\n",
    "        \n",
    "    model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "\n",
    "    all_layers = [key for key in model.module_dict.keys()]\n",
    "    # Choose just 1 layer, and register a hook there\n",
    "    if isinstance(layer, str):\n",
    "        layers = [layer]\n",
    "    elif isinstance(layer, int):\n",
    "        layers = [all_layers[layer]]\n",
    "    \n",
    "    # layers = ['features.30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) avgpool AdaptiveAvgPool2d(output_size=(7, 7))']\n",
    "    model.register_hooks(selected_layers=layers)\n",
    "\n",
    "    # Get features using only this first layer and without scaling\n",
    "    features, targets = get_features_current_layers(\n",
    "        model=model, image=image, annotations=image, scalings=[1], use_min_features=False, order=interpolate)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    features_array = features.values\n",
    "    # Get the shape of the image\n",
    "    image_shape = image.shape\n",
    "    # Reshape the features array to match the image shape and add the second dimension of features as the third dimension\n",
    "    features_image = features_array.reshape(*image_shape, -1)\n",
    "\n",
    "    # Move the last dimension to the first position\n",
    "    features_image = np.moveaxis(features_image, -1, 0)\n",
    "    # print(features.shape)\n",
    "    # print(features_image.shape)\n",
    "\n",
    "    # Now you can view the new_features using napari\n",
    "    if show_napari: napari.view_image(features_image)\n",
    "    return features_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e358c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# image = image.T\n",
    "\n",
    "# Get features of multiple (all) layers\n",
    "conv_layers = [0,2]#,5,7,10,12,14,17,19,21,24,26,28]\n",
    "all_conv = [get_layer_features(image, l) for l in conv_layers]\n",
    "\n",
    "\n",
    "### Pad first dimension of the layers with fewer features and concatenate all layers into a 4D Image\n",
    "\n",
    "# Get the shapes of all outputs\n",
    "shapes = [output.shape for output in all_conv]\n",
    "# Find the maximum shape in each dimension\n",
    "max_shape = np.max(shapes, axis=0)\n",
    "# Pad all outputs to have the max shape\n",
    "from numpy.lib import pad\n",
    "all_conv_padded = np.array([pad(output, [(0, max_dim - dim) for dim, max_dim in zip(output.shape, max_shape)]) for output in all_conv])\n",
    "\n",
    "# Show in Napari\n",
    "napari.view_image(all_conv_padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
