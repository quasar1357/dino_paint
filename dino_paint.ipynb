{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9282643-c6fa-4806-8a91-0156710c8664",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with convpaint and DINOv2\n",
    "\n",
    "This notebooks demonstrates how to run a semantic segmentation on an image using DINOv2 for feature extraction and a random forest algorithm for classification. It is based on the notebook provided by convpaint and runs independently from napari.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a1c36c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e314538e-f42d-42d0-89a9-4ddd4588c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import napari and its screenshot function\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# import what we need from conv_paint\n",
    "from napari_convpaint.conv_paint import ConvPaintWidget\n",
    "# from napari_convpaint.conv_paint_utils import Hookmodel\n",
    "from napari_convpaint.convpaint_sample import create_annotation_cell3d\n",
    "from napari_convpaint.conv_paint_utils import (filter_image_multioutputs, get_features_current_layers,\n",
    "get_multiscale_features, train_classifier, predict_image)\n",
    "from napari_convpaint.conv_paint_utils import extract_annotated_pixels\n",
    " \n",
    "# import the other general modules used\n",
    "import numpy as np\n",
    "import skimage\n",
    "# import tifffile\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import pytorch and pillow Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02abf123-f8da-4c15-8dc4-79e421f843a0",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load an image and the corresponding annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88dcd229-c9fe-4d4a-8df9-9f52cc4dc353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (504, 504, 3)\n",
      "Original label image shape: (504, 504)\n"
     ]
    }
   ],
   "source": [
    "# LOAD CELL3D IMAGE AND ANNOTATION\n",
    "# Load 3D image with 2 channels (cell borders and nuclei)\n",
    "# image_original = skimage.data.cells3d()\n",
    "# Take a layer in middle of cell (30 of 0-59) and take 2nd channel (nuclei)\n",
    "# image_original = image_original[30, 1]\n",
    "# Load annotation defined in conv_paint\n",
    "# labels_original = create_annotation_cell3d()[0][0]\n",
    "\n",
    "# Take crops of image and annotation\n",
    "# crop = ((60,188), (0,128))\n",
    "# crop = ((20,20+224), (0,224))\n",
    "# image_original = image_original[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "# labels_original = labels_original[crop[0][0]:crop[0][1], crop[1][0]:crop[1][1]]\n",
    "\n",
    "# LOAD ASTRONAUT IMAGE (RGB) AND ANNOTATION\n",
    "image_original = skimage.data.astronaut()[0:504,0:504,:]\n",
    "labels_original = plt.imread('astro_labels.tif')[0:504,0:504,0]\n",
    "\n",
    "# PRINT SHAPES\n",
    "print(f\"Original image shape: {image_original.shape}\")\n",
    "print(f\"Original label image shape: {labels_original.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c288bc18",
   "metadata": {},
   "source": [
    "Show the image and annotation as layers in napari. Print out the shape of the original image and the number of originally annotated pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88304336-c827-45d9-a27d-b554b8c486e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labels_original' at 0x2093c333160>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a napari viewer; add the image to it; add the labels/annotation\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image_original)\n",
    "viewer.add_labels(labels_original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe19519-c478-4184-b223-b448ef434c93",
   "metadata": {},
   "source": [
    "We can also show a napari screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829a161-6e6f-4e10-ba33-439b038010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot of the napari viewer here in the notebook\n",
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df1957-bfec-4cba-861d-b5f38380fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tifffile.imwrite('label_cell3d.tiff', viewer.layers['Labels'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8380ba7e-eb62-4297-92bd-827fe5ab9b41",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "DINOv2 comes in 4 different versions, each increasing in training set size and power. Choose the desired model by assigning it to 'model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50a98ea-6939-45c4-be7d-1835b7a917be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37eee0a4",
   "metadata": {},
   "source": [
    "Define the model parameters. Note that the patch size that DINOv2 uses is 14x14. Therefore, the number of patches is the input shape divided by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a691a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input used for model: 504 x 504 pixels\n",
      "Patched image shape: 36 x 36 patches\n"
     ]
    }
   ],
   "source": [
    "# Store the patch size and input shape as global constants\n",
    "PATCH_SIZE = (14, 14)\n",
    "\n",
    "# Define input shape as the smallest multiple of the patch size that is larger than the image\n",
    "IN_SHAPE = (int(np.ceil(image_original.shape[0]/14))*PATCH_SIZE[0],\n",
    "            int(np.ceil(image_original.shape[1]/14))*PATCH_SIZE[1])\n",
    "print(f\"Shape of input used for model: {IN_SHAPE[0]} x {IN_SHAPE[1]} pixels\")\n",
    "\n",
    "# Calculate the shape of the patched image (i.e. how many patches fit in the image)\n",
    "if not (IN_SHAPE[0]%PATCH_SIZE[0] == 0 and IN_SHAPE[1]%PATCH_SIZE[1] == 0):\n",
    "    raise ValueError('Input shape must be divisible by patch size')\n",
    "else:\n",
    "    patched_image_shape = (int(IN_SHAPE[0]/PATCH_SIZE[0]), int(IN_SHAPE[1]/PATCH_SIZE[1]))\n",
    "print(f\"Patched image shape: {patched_image_shape[0]} x {patched_image_shape[1]} patches\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5201b9",
   "metadata": {},
   "source": [
    "## Convert & preprocess image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8a79f-752a-4ac9-b0e9-2bb485b75338",
   "metadata": {},
   "source": [
    "Resize the image to the input shape (which has to be a multiple of the patch size 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912f6186-b239-42cd-87ec-b63907c69c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale original image to input size of the model\n",
    "image_scaled = skimage.transform.resize(image_original, IN_SHAPE, mode='edge', order=1, preserve_range=True)\n",
    "labels_scaled = skimage.transform.resize(labels_original, IN_SHAPE, mode='edge', order=0, preserve_range=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f76478",
   "metadata": {},
   "source": [
    "Show the scaled version of the image and labels to ensure that it still looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970f4d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labels_scaled' at 0x20940d13f40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(image_scaled)\n",
    "viewer.add_labels(labels_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bee4328",
   "metadata": {},
   "source": [
    "Convert the image to RGB. Then preprocess it into a torch tensor, normalized according to distribution expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87a6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RGB\n",
    "if image_scaled.ndim == 2:\n",
    "    image_rgb = np.stack((image_scaled,)*3, axis=-1)\n",
    "else:\n",
    "    image_rgb = image_scaled\n",
    "\n",
    "# New shape is (224, 224, 3); type is float64\n",
    "# print(image_rgb.shape); print(image_rgb.dtype)\n",
    "# napari.view_image(image_rgb.astype(np.int64), rgb=True)\n",
    "\n",
    "# Preprocess image and convert to PyTorch tensor\n",
    "preprocess = Compose([\n",
    "    # Resize(IN_SHAPE),  # Resize to the input size expected by the model\n",
    "    ToTensor(),  # Convert to PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to the input distribution expected by the model\n",
    "])\n",
    "\n",
    "image_pre = preprocess(image_rgb).float()\n",
    "# image_pre = preprocess(Image.fromarray(image_rgb.astype(np.uint8))).float()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7014e59",
   "metadata": {},
   "source": [
    "Show output of preprocessing to verify it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3830d356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Viewer(camera=Camera(center=(0.0, 251.5, 251.5), zoom=0.8821428571428571, angles=(0.0, 0.0, 90.0), perspective=0.0, mouse_pan=True, mouse_zoom=True), cursor=Cursor(position=(1.0, 1.0, 0.0), scaled=True, size=1, style=<CursorStyle.STANDARD: 'standard'>), dims=Dims(ndim=3, ndisplay=2, last_used=0, range=((0.0, 3.0, 1.0), (0.0, 504.0, 1.0), (0.0, 504.0, 1.0)), current_step=(1, 251, 251), order=(0, 1, 2), axis_labels=('0', '1', '2')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[<Image layer 'Image' at 0x209475b25b0>], help='use <2> for transform', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=False, mouse_move_callbacks=[], mouse_drag_callbacks=[], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[<function dims_scroll at 0x000002092D922670>], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "napari.view_image(image_pre.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cb5ea6-ba0d-4281-932e-4be4bc73abd7",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now that the model is defined, we can run an image through it and extract features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662e0575-f0a3-4cf1-87b4-6e7365273830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra batch dimension \n",
    "image_batch = image_pre.unsqueeze(0)\n",
    "\n",
    "# Move image to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# image_batch = image_batch.to(device)\n",
    "# model = model.to(device)\n",
    "\n",
    "# Pass image through the model (assuming image_batch is a batch of test images)\n",
    "with torch.no_grad():\n",
    "    features_dict = model.forward_features(image_batch)\n",
    "    features = features_dict['x_norm_patchtokens']\n",
    "\n",
    "# The output shape is [batch_size, num_patches, features] = [1, 256, NUM_FEATURES]\n",
    "# print(features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2cf84",
   "metadata": {},
   "source": [
    "Rearrange and reshape dimensions of the DINOv2 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number features: 768\n"
     ]
    }
   ],
   "source": [
    "# Read out the number of features, which is dependent on the model chosen (ViTs14 = 384; ViTb14 = 768; ViTl14 = 1024; ViTg14 = 1536)\n",
    "NUM_FEATURES = features.shape[2]\n",
    "print(f\"Number features: {NUM_FEATURES}\")\n",
    "\n",
    "# Rearrange dimensions of the feature tensor to [batch_size, features, num_patches] = [1, NUM_FEATURES, 256]\n",
    "features_perm = features.permute(0, 2, 1)\n",
    "# print(features.shape)\n",
    "\n",
    "# Reshape linear patches (256) into 2D: [batch_size, features, patches_w, patches_h] = [1, NUM_FEATURES, 16, 16]\n",
    "features_wh = features_perm.reshape(1, NUM_FEATURES, patched_image_shape[0], patched_image_shape[1])\n",
    "# print(features_wh.shape)\n",
    "\n",
    "# Upsample to original image size, i.e. [batch_size, features, image_w, image_h] = [1, NUM_FEATURES, 128, 128] or [1, NUM_FEATURES, 224, 224]\n",
    "# scaling_factor = (14 * image_original.shape[0] / 224, 14 * image_original.shape[1] / 224)\n",
    "\n",
    "# Upsample to the size of the scaled image (i.e. interpolate with scaling factor = patch_size = 14)\n",
    "features_int = torch.nn.functional.interpolate(features_wh, scale_factor=PATCH_SIZE)\n",
    "# print(features_int.shape)\n",
    "\n",
    "# Convert to numpy array and remove batch dimension to get [features, image_w, image_h] = [NUM_FEATURES, 128, 128] or [NUM_FEATURES, 224, 224]\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)\n",
    "# print(features_np.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffd8243",
   "metadata": {},
   "source": [
    "Show feature space in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11565b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'features_np' at 0x209605f4880>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the feature space\n",
    "viewer.add_image(features_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "500ba0d1",
   "metadata": {},
   "source": [
    "Extract features and target values (labels) where image is annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15bf7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of annotated features: (0, 768)\n",
      "Number of targets: 0\n",
      "Number of originally annotated pixels: 7709\n",
      "Number of annotated pixels in resized annotation: 7709\n"
     ]
    }
   ],
   "source": [
    "features_annot, targets = extract_annotated_pixels(features_np, labels_scaled)\n",
    "# features.shape = (646, NUM_FEATURES)\n",
    "# targets.shape = (646,)\n",
    "print(f\"Shape of annotated features: {features_annot.shape}\")\n",
    "print(f\"Number of targets: {targets.shape[0]}\")\n",
    "print(f\"Number of originally annotated pixels: {sum(labels_original[labels_original>0])}\")\n",
    "print(f\"Number of annotated pixels in resized annotation: {sum(labels_scaled[labels_scaled>0])}\")\n",
    "\n",
    "# # NOTE: in convpaint, we had\n",
    "# features.shape = (218, 640)\n",
    "# targets.shape = (218,)\n",
    "# And the number of originally annotated pixels was 327"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53809e60",
   "metadata": {},
   "source": [
    "## Train and use Classifier\n",
    "Finally we can train a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0faa52a6-b66d-4775-b58e-92b7ab13f5bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m random_forest \u001b[39m=\u001b[39m train_classifier(features_annot, targets)\n",
      "File \u001b[1;32mc:\\Users\\roman\\anaconda3\\envs\\dino_env\\lib\\site-packages\\napari_convpaint\\conv_paint_utils.py:810\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(features, targets)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Train a random forest classifier given a set of features and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[39m# split train/test\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m X, X_test, y, y_test \u001b[39m=\u001b[39m train_test_split(features, targets,\n\u001b[0;32m    811\u001b[0m                                         test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m    812\u001b[0m                                         random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n\u001b[0;32m    814\u001b[0m \u001b[39m# train a random forest classififer\u001b[39;00m\n\u001b[0;32m    815\u001b[0m random_forest \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\roman\\anaconda3\\envs\\dino_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\roman\\anaconda3\\envs\\dino_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2619\u001b[0m )\n\u001b[0;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\roman\\anaconda3\\envs\\dino_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2277\u001b[0m     )\n\u001b[0;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "random_forest = train_classifier(features_annot, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2ed1f9-9091-4c4f-b85d-b199c8939eba",
   "metadata": {},
   "source": [
    "And do a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93cb3004-7780-496a-b499-bc89b24bbbf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_forest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m all_features_lin \u001b[39m=\u001b[39m all_features\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Run predict of random forest on all features\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m predictions \u001b[39m=\u001b[39m random_forest\u001b[39m.\u001b[39mpredict(all_features_lin)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random_forest' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert features to numpy array\n",
    "all_features = features.numpy()\n",
    "# Remove the batch dimension\n",
    "all_features_lin = all_features.squeeze(0)\n",
    "\n",
    "# Run predict of random forest on all features\n",
    "predictions = random_forest.predict(all_features_lin)\n",
    "\n",
    "# We have 256 predictions, which corresponds to the 256 patches (16x16 in the image)\n",
    "# print(predictions.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d36fc94",
   "metadata": {},
   "source": [
    "Reshape and resize the predictions so we can show and overlay them on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e234f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the predictions to the shape of the image of patches\n",
    "predicted_image = predictions.reshape(patched_image_shape[0], patched_image_shape[1])\n",
    "# Resize to the size of the scaled input image\n",
    "predicted_image = skimage.transform.resize(predicted_image, IN_SHAPE, mode='edge', order=0, anti_aliasing=False)\n",
    "# Transform interpolated values to integer values\n",
    "predicted_image = predicted_image.astype(np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a81e79-8988-4191-a67f-344c7822a4f0",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "And finally we can visualize the output (and quantify its quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129bfefb-6769-4faf-b614-e7441f7bf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image_scaled)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels_scaled)\n",
    "# add the prediction\n",
    "viewer.add_labels(predicted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c0ddc-fac6-4509-9f10-60b0d86a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42430d5",
   "metadata": {},
   "source": [
    "## Tests Roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AND SHOW FULL OUTPUT OF A LAYER OF VGG16 (= 64 FEATURES)\n",
    "def get_layer_features(image, layer, show_napari = False, interpolate = False):\n",
    "        \n",
    "    model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "\n",
    "    all_layers = [key for key in model.module_dict.keys()]\n",
    "    # Choose just 1 layer, and register a hook there\n",
    "    if isinstance(layer, str):\n",
    "        layers = [layer]\n",
    "    elif isinstance(layer, int):\n",
    "        layers = [all_layers[layer]]\n",
    "    \n",
    "    # layers = ['features.30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) avgpool AdaptiveAvgPool2d(output_size=(7, 7))']\n",
    "    model.register_hooks(selected_layers=layers)\n",
    "\n",
    "    # Get features using only this first layer and without scaling\n",
    "    features, targets = get_features_current_layers(\n",
    "        model=model, image=image, annotations=image, scalings=[1], use_min_features=False, order=interpolate)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    features_array = features.values\n",
    "    # Get the shape of the image\n",
    "    image_shape = image.shape\n",
    "    # Reshape the features array to match the image shape and add the second dimension of features as the third dimension\n",
    "    features_image = features_array.reshape(*image_shape, -1)\n",
    "\n",
    "    # Move the last dimension to the first position\n",
    "    features_image = np.moveaxis(features_image, -1, 0)\n",
    "    # print(features.shape)\n",
    "    # print(features_image.shape)\n",
    "\n",
    "    # Now you can view the new_features using napari\n",
    "    if show_napari: napari.view_image(features_image)\n",
    "    return features_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e358c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# image = image.T\n",
    "\n",
    "# Get features of multiple (all) layers\n",
    "conv_layers = [0,2]#,5,7,10,12,14,17,19,21,24,26,28]\n",
    "all_conv = [get_layer_features(image, l) for l in conv_layers]\n",
    "\n",
    "\n",
    "### Pad first dimension of the layers with fewer features and concatenate all layers into a 4D Image\n",
    "\n",
    "# Get the shapes of all outputs\n",
    "shapes = [output.shape for output in all_conv]\n",
    "# Find the maximum shape in each dimension\n",
    "max_shape = np.max(shapes, axis=0)\n",
    "# Pad all outputs to have the max shape\n",
    "from numpy.lib import pad\n",
    "all_conv_padded = np.array([pad(output, [(0, max_dim - dim) for dim, max_dim in zip(output.shape, max_shape)]) for output in all_conv])\n",
    "\n",
    "# Show in Napari\n",
    "napari.view_image(all_conv_padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
