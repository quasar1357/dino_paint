{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9282643-c6fa-4806-8a91-0156710c8664",
   "metadata": {},
   "source": [
    "# API access to napari-convpaint\n",
    "\n",
    "This notebooks demonstrates how to run the software behind the napari-convpaint plugin completely independently from napari itself.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a1c36c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e314538e-f42d-42d0-89a9-4ddd4588c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import napari and its screenshot function\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# import what we need from conv_paint\n",
    "from napari_convpaint.conv_paint import ConvPaintWidget\n",
    "from napari_convpaint.conv_paint_utils import Hookmodel\n",
    "from napari_convpaint.convpaint_sample import create_annotation_cell3d\n",
    "from napari_convpaint.conv_paint_utils import (filter_image_multioutputs, get_features_current_layers,\n",
    "get_multiscale_features, train_classifier, predict_image)\n",
    "from napari_convpaint.conv_paint_utils import extract_annotated_pixels\n",
    " \n",
    "# import the other general modules used\n",
    "import numpy as np\n",
    "import skimage\n",
    "import tifffile\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "# import pillow.image\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02abf123-f8da-4c15-8dc4-79e421f843a0",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First we load an image and the corresponding annotation. We simply display the result as a napari screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88dcd229-c9fe-4d4a-8df9-9f52cc4dc353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load 3D image with 2 channels (cell borders and nuclei)\n",
    "image_original = skimage.data.cells3d()\n",
    "# Take layer in middle of cell (30 of 0-59) and take 2nd channel (nuclei)\n",
    "image = image_original[30,1]\n",
    "# Load annotation defined in conv_paint\n",
    "labels = create_annotation_cell3d()[0][0]\n",
    "\n",
    "# Take crops of image and annotation\n",
    "image = image[60:188, 0:128]\n",
    "labels = labels[60:188, 0:128]\n",
    "original_im_shape = image.shape\n",
    "original_im_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88304336-c827-45d9-a27d-b554b8c486e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labels' at 0x21e1af80f40>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a napari viewer\n",
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe19519-c478-4184-b223-b448ef434c93",
   "metadata": {},
   "source": [
    "This are the data we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c829a161-6e6f-4e10-ba33-439b038010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot of the napari viewer here in the notebook\n",
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11df1957-bfec-4cba-861d-b5f38380fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tifffile.imwrite('label_cell3d.tiff', viewer.layers['Labels'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8380ba7e-eb62-4297-92bd-827fe5ab9b41",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "The Hookmodel is a wrapper around PyTorch models. It allows to list all modules (layer) of a model and to place \"hooks\" in order to get the output of chosen layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b50a98ea-6939-45c4-be7d-1835b7a917be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# Choose the model to use\n",
    "model = dinov2_vits14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7700c44-ea72-4988-9b40-c7df7c3cfcde",
   "metadata": {},
   "source": [
    "If running on a CUDA GPU, it is possible to use ```model = Hookmodel(model_name='vgg16', use_cuda=True)``` to run inference on the GPU.\n",
    "\n",
    "Here is the complete list of modules of the model:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8a79f-752a-4ac9-b0e9-2bb485b75338",
   "metadata": {},
   "source": [
    "From that list we can pick specific layers that will work as feature extractors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "912f6186-b239-42cd-87ec-b63907c69c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pillow image\n",
    "image_pillow = Image.fromarray(image)\n",
    "image_pillow = image_pillow.convert(\"RGB\")\n",
    "\n",
    "# Preprocess image\n",
    "preprocess = Compose([\n",
    "    Resize((224, 224)),  # Resize to the input size expected by the model\n",
    "    ToTensor(),  # Convert to PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to ImageNet mean and std\n",
    "])\n",
    "image_pre = preprocess(image_pillow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bcbd6d2-f908-42e2-ba5a-be8c354b1860",
   "metadata": {},
   "source": [
    "Then we add hooks at these layers, i.e. we capture their output. The flow through the network is interrupted at the last selected layer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cb5ea6-ba0d-4281-932e-4be4bc73abd7",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now that the model is defined, we can run an image through it and recover the outputs at the \"hooked\" layers. All this is wrapped insisde the ```get_features_current_layers``` function.\n",
    "\n",
    "There are several options to run the feature extraction:\n",
    "- with ```use_min_features=True```, only the n-first features of each output layer are selected, n being the number of features of the layer which outputs the least of them\n",
    "- ```scalings``` specifies the levels of downscaling to use (1 is the original size)\n",
    "- ```order``` specifies the spline order used to upscale small feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "662e0575-f0a3-4cf1-87b4-6e7365273830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra batch dimension and move image to the GPU if available\n",
    "images = image_pre.unsqueeze(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images = images.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Pass image through the model (assuming images is a batch of test images)\n",
    "with torch.no_grad():\n",
    "    # features=model.forward_features(torch.nn.functional.interpolate(images,(448,448)))['x_norm_patchtokens']\n",
    "    features=model.forward_features(images)['x_norm_patchtokens']\n",
    "\n",
    "features = features.permute(0,2,1)\n",
    "features = features.reshape(1,384,16,16)#.squeeze(0)\n",
    "\n",
    "# features_int = torch.nn.functional.interpolate(features, scale_factor=14)\n",
    "features_int = torch.nn.functional.interpolate(features, size=original_im_shape)\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15bf7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(222,)\n",
      "(222, 384)\n"
     ]
    }
   ],
   "source": [
    "features, targets = extract_annotated_pixels(features_np, labels)\n",
    "print(targets.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cab5387-0e8f-4611-838c-14b4323a7ab3",
   "metadata": {},
   "source": [
    "The selected output layers have 64 and 256 output features, for a total of 320 features. This matches the number of features of the ```features``` dataframe as shown above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53809e60",
   "metadata": {},
   "source": [
    "## Train and use Classifier\n",
    "Finally we can train a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0faa52a6-b66d-4775-b58e-92b7ab13f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_classifier(features, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2ed1f9-9091-4c4f-b85d-b199c8939eba",
   "metadata": {},
   "source": [
    "And do a prediction. Note that the same settings as those used for training need to be used here for ```scalings```, ```order``` and ```use_min_features```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93cb3004-7780-496a-b499-bc89b24bbbf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predicted \u001b[39m=\u001b[39m predict_image(image, model, random_forest, scalings\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m], order\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, use_min_features\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\roman\\anaconda3\\envs\\dino_env\\lib\\site-packages\\napari_convpaint\\conv_paint_utils.py:175\u001b[0m, in \u001b[0;36mpredict_image\u001b[1;34m(image, model, classifier, scalings, order, use_min_features, device)\u001b[0m\n\u001b[0;32m    171\u001b[0m     tot_filters \u001b[39m=\u001b[39m max_features \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(all_scales)\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[39m# max_features = np.max(model.features_per_layer)\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     all_scales \u001b[39m=\u001b[39m filter_image_multioutputs(\n\u001b[0;32m    176\u001b[0m         image, model, scalings\u001b[39m=\u001b[39;49mscalings, order\u001b[39m=\u001b[39;49morder, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    177\u001b[0m     \u001b[39m# MV:  why this?         (64, 256, 512)                2            / 3\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     tot_filters \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(model\u001b[39m.\u001b[39mfeatures_per_layer) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(all_scales) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mfeatures_per_layer)\n",
      "File \u001b[1;32mc:\\Users\\roman\\anaconda3\\envs\\dino_env\\lib\\site-packages\\napari_convpaint\\conv_paint_utils.py:97\u001b[0m, in \u001b[0;36mfilter_image_multioutputs\u001b[1;34m(image, hookmodel, scalings, order, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Recover the outputs of chosen layers of a pytorch model. Layers and model are\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mspecified in the hookmodel object.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m is_model_cuda \u001b[39m=\u001b[39m device \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 97\u001b[0m input_channels \u001b[39m=\u001b[39m hookmodel\u001b[39m.\u001b[39;49mnamed_modules[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39min_channels\n\u001b[0;32m     98\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(image, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     99\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((input_channels, image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32) \u001b[39m*\u001b[39m image\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "predicted = predict_image(image, model, random_forest, scalings=[1,2], order=1, use_min_features=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a81e79-8988-4191-a67f-344c7822a4f0",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "And finally we can visualize the output (and quantify its quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "129bfefb-6769-4faf-b614-e7441f7bf9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'predicted' at 0x2d707268d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.add_labels(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c0ddc-fac6-4509-9f10-60b0d86a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42430d5",
   "metadata": {},
   "source": [
    "## Tests Roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(labels)\n",
    "\n",
    "np.sum(labels > 0)\n",
    "\n",
    "# plt.imshow(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AND SHOW FULL OUTPUT OF A LAYER OF VGG16 (= 64 FEATURES)\n",
    "def get_layer_features(image, layer, show_napari = False, interpolate = False):\n",
    "        \n",
    "    model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "\n",
    "    all_layers = [key for key in model.module_dict.keys()]\n",
    "    # Choose just 1 layer, and register a hook there\n",
    "    if isinstance(layer, str):\n",
    "        layers = [layer]\n",
    "    elif isinstance(layer, int):\n",
    "        layers = [all_layers[layer]]\n",
    "    \n",
    "    # layers = ['features.30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) avgpool AdaptiveAvgPool2d(output_size=(7, 7))']\n",
    "    model.register_hooks(selected_layers=layers)\n",
    "\n",
    "    # Get features using only this first layer and without scaling\n",
    "    features, targets = get_features_current_layers(\n",
    "        model=model, image=image, annotations=image, scalings=[1], use_min_features=False, order=interpolate)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    features_array = features.values\n",
    "    # Get the shape of the image\n",
    "    image_shape = image.shape\n",
    "    # Reshape the features array to match the image shape and add the second dimension of features as the third dimension\n",
    "    features_image = features_array.reshape(*image_shape, -1)\n",
    "\n",
    "    # Move the last dimension to the first position\n",
    "    features_image = np.moveaxis(features_image, -1, 0)\n",
    "    # print(features.shape)\n",
    "    # print(features_image.shape)\n",
    "\n",
    "    # Now you can view the new_features using napari\n",
    "    if show_napari: napari.view_image(features_image)\n",
    "    return features_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e358c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Viewer(camera=Camera(center=(0.0, 63.5, 63.5), zoom=3.350573501872659, angles=(0.0, 0.0, 90.0), perspective=0.0, mouse_pan=True, mouse_zoom=True), cursor=Cursor(position=(0.0, 31.0, 0.0, 0.0), scaled=True, size=1, style=<CursorStyle.STANDARD: 'standard'>), dims=Dims(ndim=4, ndisplay=2, last_used=0, range=((0.0, 2.0, 1.0), (0.0, 64.0, 1.0), (0.0, 128.0, 1.0), (0.0, 128.0, 1.0)), current_step=(0, 31, 63, 63), order=(0, 1, 2, 3), axis_labels=('0', '1', '2', '3')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[<Image layer 'all_conv_padded' at 0x1b05f5a8130>], help='use <2> for transform', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=False, mouse_move_callbacks=[], mouse_drag_callbacks=[], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[<function dims_scroll at 0x000001B050F6B280>], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN\n",
    "\n",
    "# image = image.T\n",
    "\n",
    "# Get features of multiple (all) layers\n",
    "conv_layers = [0,2]#,5,7,10,12,14,17,19,21,24,26,28]\n",
    "all_conv = [get_layer_features(image, l) for l in conv_layers]\n",
    "\n",
    "\n",
    "### Pad first dimension of the layers with fewer features and concatenate all layers into a 4D Image\n",
    "\n",
    "# Get the shapes of all outputs\n",
    "shapes = [output.shape for output in all_conv]\n",
    "# Find the maximum shape in each dimension\n",
    "max_shape = np.max(shapes, axis=0)\n",
    "# Pad all outputs to have the max shape\n",
    "from numpy.lib import pad\n",
    "all_conv_padded = np.array([pad(output, [(0, max_dim - dim) for dim, max_dim in zip(output.shape, max_shape)]) for output in all_conv])\n",
    "\n",
    "# Show in Napari\n",
    "napari.view_image(all_conv_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# SHOW VGG16 MODEL SUMMARY IN PYTORCH\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load vgg16 model\n",
    "vgg16 = models.vgg16()\n",
    "\n",
    "# Print model summary\n",
    "print(vgg16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
