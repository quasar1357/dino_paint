{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9282643-c6fa-4806-8a91-0156710c8664",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with convpaint and DINOv2\n",
    "\n",
    "This notebooks demonstrates how to run a semantic segmentation on an image using DINOv2 for feature extraction and a random forest algorithm for classification. It is based on the notebook provided by convpaint and runs independently from napari.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a1c36c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e314538e-f42d-42d0-89a9-4ddd4588c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import napari and its screenshot function\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# import what we need from conv_paint\n",
    "from napari_convpaint.conv_paint import ConvPaintWidget\n",
    "from napari_convpaint.conv_paint_utils import Hookmodel\n",
    "from napari_convpaint.convpaint_sample import create_annotation_cell3d\n",
    "from napari_convpaint.conv_paint_utils import (filter_image_multioutputs, get_features_current_layers,\n",
    "get_multiscale_features, train_classifier, predict_image)\n",
    "from napari_convpaint.conv_paint_utils import extract_annotated_pixels\n",
    " \n",
    "# import the other general modules used\n",
    "import numpy as np\n",
    "import skimage\n",
    "import tifffile\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "# import pillow.image\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02abf123-f8da-4c15-8dc4-79e421f843a0",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load an image and the corresponding annotation. Both are cropped to be 128x128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88dcd229-c9fe-4d4a-8df9-9f52cc4dc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3D image with 2 channels (cell borders and nuclei)\n",
    "image_original = skimage.data.cells3d()\n",
    "# Take a layer in middle of cell (30 of 0-59) and take 2nd channel (nuclei)\n",
    "image = image_original[30,1]\n",
    "# Load annotation defined in conv_paint\n",
    "labels = create_annotation_cell3d()[0][0]\n",
    "\n",
    "# Take crops of image and annotation\n",
    "image = image[60:188, 0:128]\n",
    "labels = labels[60:188, 0:128]\n",
    "original_im_shape = image.shape\n",
    "# The original image 'cells3d' is 128x128 pixels\n",
    "# print(original_im_shape)\n",
    "# The number of annotated pixels is 327\n",
    "# print(sum(labels[labels>0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c288bc18",
   "metadata": {},
   "source": [
    "Show the image and annotation as layers in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88304336-c827-45d9-a27d-b554b8c486e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labels' at 0x1b7b8cfbf10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a napari viewer\n",
    "viewer = napari.Viewer()\n",
    "# add the loaded image to it\n",
    "viewer.add_image(image)\n",
    "# add the loaded labels/annotation\n",
    "viewer.add_labels(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe19519-c478-4184-b223-b448ef434c93",
   "metadata": {},
   "source": [
    "Instead we can show a napari screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c829a161-6e6f-4e10-ba33-439b038010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a screenshot of the napari viewer here in the notebook\n",
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11df1957-bfec-4cba-861d-b5f38380fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tifffile.imwrite('label_cell3d.tiff', viewer.layers['Labels'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8380ba7e-eb62-4297-92bd-827fe5ab9b41",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "DINOv2 comes in 4 different versions, each increasing in training set size and power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b50a98ea-6939-45c4-be7d-1835b7a917be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\roman/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# Choose the model to use\n",
    "model = dinov2_vits14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c5201b9",
   "metadata": {},
   "source": [
    "## Convert & preprocess image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e8a79f-752a-4ac9-b0e9-2bb485b75338",
   "metadata": {},
   "source": [
    "We convert the image to a pillow format RGB image. Then it is preprocessed into a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912f6186-b239-42cd-87ec-b63907c69c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pillow image and make RGB\n",
    "image_pillow = Image.fromarray(image)\n",
    "image_pillow = image_pillow.convert(\"RGB\")\n",
    "\n",
    "# Preprocess image\n",
    "preprocess = Compose([\n",
    "    Resize((224, 224)),  # Resize to the input size expected by the model\n",
    "    ToTensor(),  # Convert to PyTorch tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize to ImageNet mean and std\n",
    "])\n",
    "image_pre = preprocess(image_pillow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73cb5ea6-ba0d-4281-932e-4be4bc73abd7",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now that the model is defined, we can run an image through it and extract features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "662e0575-f0a3-4cf1-87b4-6e7365273830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an extra batch dimension and move image to the GPU if available\n",
    "images = image_pre.unsqueeze(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images = images.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Pass image through the model (assuming images is a batch of test images)\n",
    "with torch.no_grad():\n",
    "    # features=model.forward_features(torch.nn.functional.interpolate(images,(448,448)))['x_norm_patchtokens']\n",
    "    features=model.forward_features(images)['x_norm_patchtokens']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d2cf84",
   "metadata": {},
   "source": [
    "Rearrange and reshape dimensions of the DINOv2 output. Note that the patch size that DINOv2 uses is 14x14. Therefore, the number of patches is the input shape divided by 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e533190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 16, 16])\n",
      "torch.Size([1, 384, 128, 128])\n",
      "(384, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# The output shape is [batch_size, patches, features] = [1, 256, 384]\n",
    "# print(features.shape)\n",
    "\n",
    "# Rearrange dimensions of the feature tensor to [batch_size, features, patches] = [1, 384, 256]\n",
    "features_perm = features.permute(0,2,1)\n",
    "# print(features.shape)\n",
    "\n",
    "# Reshape patches (256) into width and height --> [1, 384, 16, 16]\n",
    "features_wh = features_perm.reshape(1,384,16,16)\n",
    "print(features_wh.shape)\n",
    "\n",
    "# Upsample to original image size, i.e. [batch_size, features, image_w, image_h] = [1, 384, 128, 128]\n",
    "fact = (14 * original_im_shape[0] / 224, 14 * original_im_shape[1] / 224)\n",
    "features_int = torch.nn.functional.interpolate(features_wh, scale_factor=fact)\n",
    "print(features_int.shape)\n",
    "\n",
    "# Convert to numpy array and remove batch dimension to get [features, image_w, image_h] = [384, 128, 128]\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)\n",
    "print(features_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15bf7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(222, 384)\n",
      "(222,)\n"
     ]
    }
   ],
   "source": [
    "features_annot, targets = extract_annotated_pixels(features_np, labels)\n",
    "# features.shape = (222, 384)\n",
    "# targets.shape = (222,)\n",
    "# print(features.shape)\n",
    "# print(targets.shape)\n",
    "# # NOTE: in convpaint, we had\n",
    "# features.shape = (218, 640)\n",
    "# targets.shape = (218,)\n",
    "# And the number of annotated pixels was 327"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53809e60",
   "metadata": {},
   "source": [
    "## Train and use Classifier\n",
    "Finally we can train a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0faa52a6-b66d-4775-b58e-92b7ab13f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = train_classifier(features_annot, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e2ed1f9-9091-4c4f-b85d-b199c8939eba",
   "metadata": {},
   "source": [
    "And do a prediction. Note that the same settings as those used for training need to be used here for ```scalings```, ```order``` and ```use_min_features```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93cb3004-7780-496a-b499-bc89b24bbbf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# predicted = predict_image(image, model, random_forest, scalings=[1,2], order=1, #use_min_features=False)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# all_pixels = pd.DataFrame(\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#     np.reshape(np.concatenate(all_scales, axis=1), newshape=(tot_filters, image.shape[0] * image.shape[1])).T)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m features_perm_np \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m     14\u001b[0m features_lin \u001b[39m=\u001b[39m features_perm_np\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     15\u001b[0m predictions \u001b[39m=\u001b[39m random_forest\u001b[39m.\u001b[39mpredict(features_lin)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "# predicted = predict_image(image, model, random_forest, scalings=[1,2], order=1, #use_min_features=False)\n",
    "\n",
    "\n",
    "# # max_features = np.max(model.features_per_layer)\n",
    "# all_scales = filter_image_multioutputs(\n",
    "#     image, model, scalings=scalings, order=order, device=device)\n",
    "# MV:  why this?         (64, 256, 512)                2            / 3\n",
    "# tot_filters = np.sum(model.features_per_layer) * len(all_scales) / len(model.features_per_layer)\n",
    "# tot_filters = int(tot_filters)\n",
    "# all_pixels = pd.DataFrame(\n",
    "#     np.reshape(np.concatenate(all_scales, axis=1), newshape=(tot_filters, image.shape[0] * image.shape[1])).T)\n",
    "\n",
    "############################\n",
    "\n",
    "# The output shape is [batch_size, patches, features] = [1, 256, 384]\n",
    "# print(features.shape)\n",
    "\n",
    "# Rearrange dimensions of the feature tensor to [batch_size, features, patches] = [1, 384, 256]\n",
    "features_perm = features.permute(0,2,1)\n",
    "# print(features.shape)\n",
    "\n",
    "# Reshape patches (256) into width and height\n",
    "features_wh = features_perm.reshape(1,384,16,16)\n",
    "print(features_wh.shape)\n",
    "\n",
    "# Upsample to original image size, i.e. [batch_size, features, image_w, image_h] = [1, 384, 128, 128]\n",
    "# SHOULD BE 128x128 !!!\n",
    "fact = (14 * original_im_shape[0] / 224, 14 * original_im_shape[1] / 224)\n",
    "features_int = torch.nn.functional.interpolate(features_wh, scale_factor=fact)\n",
    "print(features_int.shape)\n",
    "\n",
    "# Convert to numpy array and remove batch dimension to get [features, image_w, image_h] = [384, 128, 128]\n",
    "features_np = features_int.numpy()\n",
    "features_np = np.squeeze(features_np, axis=0)\n",
    "print(features_np.shape)\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "features = features.numpy()\n",
    "features_lin = features_perm_np.squeeze(0)\n",
    "predictions = random_forest.predict(features_lin)\n",
    "\n",
    "predicted_image = np.reshape(predictions, image.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88a81e79-8988-4191-a67f-344c7822a4f0",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "And finally we can visualize the output (and quantify its quality):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "129bfefb-6769-4faf-b614-e7441f7bf9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'predicted' at 0x2d707268d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.add_labels(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c0ddc-fac6-4509-9f10-60b0d86a5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbscreenshot(viewer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d42430d5",
   "metadata": {},
   "source": [
    "## Tests Roman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(labels)\n",
    "\n",
    "np.sum(labels > 0)\n",
    "\n",
    "# plt.imshow(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE AND SHOW FULL OUTPUT OF A LAYER OF VGG16 (= 64 FEATURES)\n",
    "def get_layer_features(image, layer, show_napari = False, interpolate = False):\n",
    "        \n",
    "    model = Hookmodel(model_name='vgg16')\n",
    "\n",
    "\n",
    "    all_layers = [key for key in model.module_dict.keys()]\n",
    "    # Choose just 1 layer, and register a hook there\n",
    "    if isinstance(layer, str):\n",
    "        layers = [layer]\n",
    "    elif isinstance(layer, int):\n",
    "        layers = [all_layers[layer]]\n",
    "    \n",
    "    # layers = ['features.30 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) avgpool AdaptiveAvgPool2d(output_size=(7, 7))']\n",
    "    model.register_hooks(selected_layers=layers)\n",
    "\n",
    "    # Get features using only this first layer and without scaling\n",
    "    features, targets = get_features_current_layers(\n",
    "        model=model, image=image, annotations=image, scalings=[1], use_min_features=False, order=interpolate)\n",
    "\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    features_array = features.values\n",
    "    # Get the shape of the image\n",
    "    image_shape = image.shape\n",
    "    # Reshape the features array to match the image shape and add the second dimension of features as the third dimension\n",
    "    features_image = features_array.reshape(*image_shape, -1)\n",
    "\n",
    "    # Move the last dimension to the first position\n",
    "    features_image = np.moveaxis(features_image, -1, 0)\n",
    "    # print(features.shape)\n",
    "    # print(features_image.shape)\n",
    "\n",
    "    # Now you can view the new_features using napari\n",
    "    if show_napari: napari.view_image(features_image)\n",
    "    return features_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e358c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Viewer(camera=Camera(center=(0.0, 63.5, 63.5), zoom=3.350573501872659, angles=(0.0, 0.0, 90.0), perspective=0.0, mouse_pan=True, mouse_zoom=True), cursor=Cursor(position=(0.0, 31.0, 0.0, 0.0), scaled=True, size=1, style=<CursorStyle.STANDARD: 'standard'>), dims=Dims(ndim=4, ndisplay=2, last_used=0, range=((0.0, 2.0, 1.0), (0.0, 64.0, 1.0), (0.0, 128.0, 1.0), (0.0, 128.0, 1.0)), current_step=(0, 31, 63, 63), order=(0, 1, 2, 3), axis_labels=('0', '1', '2', '3')), grid=GridCanvas(stride=1, shape=(-1, -1), enabled=False), layers=[<Image layer 'all_conv_padded' at 0x1b05f5a8130>], help='use <2> for transform', status='Ready', tooltip=Tooltip(visible=False, text=''), theme='dark', title='napari', mouse_over_canvas=False, mouse_move_callbacks=[], mouse_drag_callbacks=[], mouse_double_click_callbacks=[], mouse_wheel_callbacks=[<function dims_scroll at 0x000001B050F6B280>], _persisted_mouse_event={}, _mouse_drag_gen={}, _mouse_wheel_gen={}, keymap={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN\n",
    "\n",
    "# image = image.T\n",
    "\n",
    "# Get features of multiple (all) layers\n",
    "conv_layers = [0,2]#,5,7,10,12,14,17,19,21,24,26,28]\n",
    "all_conv = [get_layer_features(image, l) for l in conv_layers]\n",
    "\n",
    "\n",
    "### Pad first dimension of the layers with fewer features and concatenate all layers into a 4D Image\n",
    "\n",
    "# Get the shapes of all outputs\n",
    "shapes = [output.shape for output in all_conv]\n",
    "# Find the maximum shape in each dimension\n",
    "max_shape = np.max(shapes, axis=0)\n",
    "# Pad all outputs to have the max shape\n",
    "from numpy.lib import pad\n",
    "all_conv_padded = np.array([pad(output, [(0, max_dim - dim) for dim, max_dim in zip(output.shape, max_shape)]) for output in all_conv])\n",
    "\n",
    "# Show in Napari\n",
    "napari.view_image(all_conv_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# SHOW VGG16 MODEL SUMMARY IN PYTORCH\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load vgg16 model\n",
    "vgg16 = models.vgg16()\n",
    "\n",
    "# Print model summary\n",
    "print(vgg16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
